# Chat CSV App

A modern web application that allows you to chat with your CSV files using AI. Built with FastAPI, PandasAI v3, and a beautiful HTML/JS frontend.

## Features

- ðŸ¤– **Multiple LLM Providers**: Support for OpenAI, Google Gemini, and Ollama
- ðŸ“Š **CSV File Upload**: Upload single or multiple CSV files with drag-and-drop support
- ðŸ’¬ **Natural Language Queries**: Ask questions about your data in plain English
- ðŸ“ˆ **Data Visualization**: Generate charts and plots from your data
- ðŸŽ¨ **Modern UI**: Beautiful, responsive interface with step-by-step guidance
- ðŸ”„ **Dynamic LLM Switching**: Change AI providers on the fly

## Requirements

- Docker and Docker Compose

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd chat_csv_app
```

2. Build and run with Docker Compose:
```bash
docker-compose up --build
```

3. Open your browser and go to `http://localhost:8080`

## Important: Using Ollama

If you plan to use Ollama as your AI provider:

1. **Install Ollama on your host machine** (not in the container):
   ```bash
   # On macOS
   brew install ollama
   # Or download from https://ollama.ai
   ```

2. **Start Ollama on your host machine**:
   ```bash
   ollama serve
   ```

3. **Pull the models you want to use**:
   ```bash
   ollama pull llama3.2
   ollama pull mistral
   ollama pull codellama
   ```

4. **Keep Ollama running** while using the containerized app

The containerized app will connect to Ollama running on your host machine via `host.docker.internal:11434`.

## Usage

Follow the 3-step process:
   - **Step 1**: Configure your AI model (OpenAI, Gemini, or Ollama)
   - **Step 2**: Upload your CSV files
   - **Step 3**: Start chatting with your data!

## Supported LLM Providers

### OpenAI
- Models: GPT-4, GPT-4 Turbo, GPT-3.5 Turbo
- Requires: OpenAI API key

### Google Gemini
- Models: Gemini Pro, Gemini Pro Vision
- Requires: Google API key

### Ollama
- Models: Llama2, CodeLlama, Mistral, Llama3.2, Phi3, etc.
- Requires: Local Ollama installation running on host machine (no API key needed)
- **Important**: Ollama must be running on your host machine (not in container)

## Example Queries

Once you've uploaded your CSV files, you can ask questions like:

- "What is the average revenue by region?"
- "Show me the top 10 customers by sales"
- "Create a bar chart of monthly sales"
- "What are the trends in the data?"
- "Find outliers in the dataset"
- "Calculate the correlation between price and sales"

## API Endpoints

- `GET /` - Serve the main HTML page
- `GET /api/llm-providers` - Get available LLM providers
- `POST /api/configure-llm` - Configure the LLM provider
- `POST /api/upload-csv` - Upload CSV files
- `GET /api/uploaded-files` - Get information about uploaded files
- `POST /api/chat` - Chat with the data

## Project Structure

```
chat_csv_app/
â”œâ”€â”€ main.py              # FastAPI backend application
â”œâ”€â”€ pyproject.toml       # Poetry configuration
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html      # Main HTML page
â”‚   â””â”€â”€ app.js          # Frontend JavaScript
â””â”€â”€ sample_data/        # Sample CSV files for testing
```

## Development

To make changes to the application:

1. Edit your code
2. Rebuild and restart the containers:
```bash
docker-compose up --build --force-recreate
```

For development with live code changes, you can mount the source code as a volume by adding this to the `docker-compose.yml`:
```yaml
volumes:
  - .:/app
  - ./exports:/app/exports
```

## Docker Information

### What's Included in the Docker Setup

- **Multi-stage optimized build**: Uses Python 3.11-slim for smaller image size
- **Poetry dependency management**: Installs only production dependencies
- **Volume mounting**: Persists generated charts in `exports/` directory
- **Health checks**: Monitors application health
- **Port mapping**: Maps container port 8080 to host port 8080

### Docker Commands Reference

```bash
# Build and start the application
docker-compose up --build

# Run in background (detached mode)
docker-compose up --build -d

# Stop and remove containers
docker-compose down

# View logs
docker-compose logs -f chat-csv-app

# Rebuild after code changes
docker-compose up --build --force-recreate
```

### Using with Ollama (Local AI Models)

If you want to use Ollama for local AI models, uncomment the Ollama service in `docker-compose.yml`:

1. Uncomment the `ollama` service section
2. Run: `docker-compose up --build`
3. Pull models: `docker exec -it ollama ollama pull llama3.2`
4. Use `localhost:11434` as the Ollama endpoint in the app

## Troubleshooting

1. **LLM Configuration Issues**: Make sure you have the correct API key for your chosen provider
2. **File Upload Problems**: Ensure your files are in CSV format and not corrupted
3. **Chat Not Working**: Verify that both LLM is configured and CSV files are uploaded
4. **Ollama Issues**: 
   - Make sure Ollama is installed and running on your **host machine** (not in container)
   - Run `ollama serve` in a terminal on your host machine
   - Verify Ollama is accessible at `http://localhost:11434` on your host
   - Make sure you've pulled the model: `ollama pull <model-name>`
5. **Docker Issues**: 
   - Make sure Docker is installed and running
   - Check if port 8080 is available
   - Verify volume mounting permissions for exports directory
   - For Ollama connection issues, ensure `host.docker.internal` is accessible from container

## License

This project is open source and available under the MIT License.
